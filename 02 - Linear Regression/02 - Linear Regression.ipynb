{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Exercise\n",
    "\n",
    "This exercise will guide you in implementing the Linear Regression Model to gain intuitions and develop a deeper understanding of the model. These concepts will form as the foundation for more complex models later on.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a Linear Regression Model.\n",
    "    - Implement the Analytical solution for the parameters that minimizes the loss function\n",
    "    - Implement the Approximate / Iterative solution for finding the parameters that minimizes the Loss function \n",
    "        - Initializing Parameters\n",
    "        - Calculating the Cost/Loss/Objective Function\n",
    "        - Computing for the gradients of the Loss function with respect to the parameters\n",
    "        - Implement gradient descent to update the paramters\n",
    "        \n",
    "**General Instructions:**\n",
    "- You may add your own variables but you may not delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use the sales prices of houses in Kansas City as our dataset. It has 21 columns:\n",
    "\n",
    "1. **id** - (numeric) unique id for each house\n",
    "2. **date** - (string) Date house was sold\n",
    "3. **price** - (numeric) Price at which the house was sold\n",
    "4. **bedrooms** - (numeric) Number of bedrooms\n",
    "5. **bathrooms** - (numeric) Number of bathrooms  / bedrooms\n",
    "6. **sqft_living** - (numeric) Square footage of the house\n",
    "7. **sqft_lot** - (numeric) Square footage of the lot\n",
    "8. **floors** - (numeric) Total floors (levels) in the house\n",
    "9. **waterfront** - (numeric) House which has a view to a waterfront\n",
    "10. **view** - (numeric) Has been viewed (binary)\n",
    "11. **condition** - (numeric) How good he condition is overall\n",
    "12. **grade** - (numeric)  Overall grade given to the housing unit, based on King County grading system\n",
    "13. **sqft_above** - (numeric) Square footage of house apart from basement\n",
    "14. **sqft_basement** - (numeric) Square footage of the basement\n",
    "15. **yr_built** - (numeric) Year the house was built\n",
    "16. **yr_renovated** - (numeric) Year when the house was renovated\n",
    "17. **zipcode** - (numeric) Zip-code\n",
    "18. **lat** - (numeric) Latitude coordinate\n",
    "19. **long** - (numeric) Longitude coordinate\n",
    "20. **sqft_living15** - (numeric) Living room area in 2015(implies-- some renovations). This might or might not have affected the lotsize area\n",
    "21. **sqft_lot15** - (numeric) LotSize area in 2015(implies-- some renovations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the raw data (21613, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads the csv file and converts it into a pandas dataframe\n",
    "rawData = pd.read_csv('kc_house_data.csv')\n",
    "print(\"Shape of the raw data\", rawData.shape)\n",
    "\n",
    "# displays the first 5 entries\n",
    "rawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the price of a house using the information available (i.e. all the columns except for the price). For now, we will only use one feature / variable (sqft_living) to be able to visualize the dataset in a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.580302e+09</td>\n",
       "      <td>5.400881e+05</td>\n",
       "      <td>3.370842</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>2079.899736</td>\n",
       "      <td>1.510697e+04</td>\n",
       "      <td>1.494309</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.234303</td>\n",
       "      <td>3.409430</td>\n",
       "      <td>7.656873</td>\n",
       "      <td>1788.390691</td>\n",
       "      <td>291.509045</td>\n",
       "      <td>1971.005136</td>\n",
       "      <td>84.402258</td>\n",
       "      <td>98077.939805</td>\n",
       "      <td>47.560053</td>\n",
       "      <td>-122.213896</td>\n",
       "      <td>1986.552492</td>\n",
       "      <td>12768.455652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.876566e+09</td>\n",
       "      <td>3.671272e+05</td>\n",
       "      <td>0.930062</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>918.440897</td>\n",
       "      <td>4.142051e+04</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>1.175459</td>\n",
       "      <td>828.090978</td>\n",
       "      <td>442.575043</td>\n",
       "      <td>29.373411</td>\n",
       "      <td>401.679240</td>\n",
       "      <td>53.505026</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>685.391304</td>\n",
       "      <td>27304.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.123049e+09</td>\n",
       "      <td>3.219500e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1427.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.471000</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.904930e+09</td>\n",
       "      <td>4.500000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98065.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.230000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.308900e+09</td>\n",
       "      <td>6.450000e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068800e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+09</td>\n",
       "      <td>7.700000e+06</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price      bedrooms     bathrooms   sqft_living  \\\n",
       "count  2.161300e+04  2.161300e+04  21613.000000  21613.000000  21613.000000   \n",
       "mean   4.580302e+09  5.400881e+05      3.370842      2.114757   2079.899736   \n",
       "std    2.876566e+09  3.671272e+05      0.930062      0.770163    918.440897   \n",
       "min    1.000102e+06  7.500000e+04      0.000000      0.000000    290.000000   \n",
       "25%    2.123049e+09  3.219500e+05      3.000000      1.750000   1427.000000   \n",
       "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
       "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
       "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
       "\n",
       "           sqft_lot        floors    waterfront          view     condition  \\\n",
       "count  2.161300e+04  21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean   1.510697e+04      1.494309      0.007542      0.234303      3.409430   \n",
       "std    4.142051e+04      0.539989      0.086517      0.766318      0.650743   \n",
       "min    5.200000e+02      1.000000      0.000000      0.000000      1.000000   \n",
       "25%    5.040000e+03      1.000000      0.000000      0.000000      3.000000   \n",
       "50%    7.618000e+03      1.500000      0.000000      0.000000      3.000000   \n",
       "75%    1.068800e+04      2.000000      0.000000      0.000000      4.000000   \n",
       "max    1.651359e+06      3.500000      1.000000      4.000000      5.000000   \n",
       "\n",
       "              grade    sqft_above  sqft_basement      yr_built  yr_renovated  \\\n",
       "count  21613.000000  21613.000000   21613.000000  21613.000000  21613.000000   \n",
       "mean       7.656873   1788.390691     291.509045   1971.005136     84.402258   \n",
       "std        1.175459    828.090978     442.575043     29.373411    401.679240   \n",
       "min        1.000000    290.000000       0.000000   1900.000000      0.000000   \n",
       "25%        7.000000   1190.000000       0.000000   1951.000000      0.000000   \n",
       "50%        7.000000   1560.000000       0.000000   1975.000000      0.000000   \n",
       "75%        8.000000   2210.000000     560.000000   1997.000000      0.000000   \n",
       "max       13.000000   9410.000000    4820.000000   2015.000000   2015.000000   \n",
       "\n",
       "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
       "count  21613.000000  21613.000000  21613.000000   21613.000000   21613.000000  \n",
       "mean   98077.939805     47.560053   -122.213896    1986.552492   12768.455652  \n",
       "std       53.505026      0.138564      0.140828     685.391304   27304.179631  \n",
       "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
       "25%    98033.000000     47.471000   -122.328000    1490.000000    5100.000000  \n",
       "50%    98065.000000     47.571800   -122.230000    1840.000000    7620.000000  \n",
       "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
       "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common summary statistics\n",
    "rawData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIPLab-Ti-6\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# this gets the columns \"sqft living\" and \"price\"\n",
    "data = rawData[['sqft_living', 'price']]\n",
    "data.head()\n",
    "\n",
    "# convert the pandas dataframe to numpy array\n",
    "# depending on your version .as_matrix() might not work\n",
    "# try .to_numpy()\n",
    "data = data.as_matrix()\n",
    "#data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will separate our dataset into 2 datasets: the **train** and **test** datasets.\n",
    "\n",
    "The linear regression model will look for a good set of **weights/theta** after looking at the train dataset. After training, we will confirm with the never-before-trained-on test set and see if the weights we learned will still perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (15129, 1)\n",
      "Training ground truth values shape: (15129, 1)\n",
      "Validation data shape: (3242, 1)\n",
      "Validation ground truth values shape: (3242, 1)\n",
      "Test data shape: (3242, 1)\n",
      "Test ground truth values shape: (3242, 1)\n"
     ]
    }
   ],
   "source": [
    "num_data = data.shape[0]\n",
    "\n",
    "# Split the dataset into train, val, and test\n",
    "train_percentage = 0.7\n",
    "num_train = np.floor(num_data*train_percentage).astype(int)\n",
    "num_val = (num_data - num_train) // 2\n",
    "num_test = (num_data - num_train) // 2\n",
    "\n",
    "# numpy.expand_dims inserts a new axis / dimension. \n",
    "# This is makes it more convenient to process later on\n",
    "# doc url: https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html\n",
    "X_train = np.expand_dims(data[0:num_train,0], -1)\n",
    "y_train = np.expand_dims(data[0:num_train,1], -1)\n",
    "\n",
    "X_val = np.expand_dims(data[num_train:num_train+num_val,0], -1)\n",
    "y_val = np.expand_dims(data[num_train:num_train+num_val,1], -1)\n",
    "\n",
    "X_test = np.expand_dims(data[num_train+num_val:num_train+num_val+num_test,0], -1)\n",
    "y_test = np.expand_dims(data[num_train+num_val:num_train+num_val+num_test,1], -1)\n",
    "\n",
    "print('Training data shape:',X_train.shape)\n",
    "print('Training ground truth values shape:',y_train.shape)\n",
    "\n",
    "print('Validation data shape:',X_val.shape)\n",
    "print('Validation ground truth values shape:',y_val.shape)\n",
    "\n",
    "print('Test data shape:',X_test.shape)\n",
    "print('Test ground truth values shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1500000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the data\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see the (positive) linear relationship between the living area of the house and its price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "$X \\in \\mathbb{R}^{N,D}$ - our data is represented as a matrix with $N$ rows and $D$ columns, where each row is a $D$-dimensional feature vector representing an instance / example in our dataset.\n",
    "\n",
    "$y \\in \\mathbb{R}^N$ - the prediction target is represented as a vector of length $N$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Analytical solution/Closed form/Normal Equation\n",
    "\n",
    "We can quickly compute for the weights by getting the derivative of our objective function and equating it to 0. However, there are some drawbacks to using this method, as you would see in the following cells.\n",
    "\n",
    "The next cells show the step-by-step process of implementing the closed form solution of linear regression:\n",
    "1. Appending $w$ in the weight vector $W$ and $x$ in $X$ to account for the bias/off-set term.\n",
    "1. Finding for the optimal values of weight vector W using the normal equation.\n",
    "3. Do some predictions! Now that we have the weights, the label can be solved by plugging in the features $x$ into the linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pre-processing! Add in a vector of one to X to account for the bias\n",
    "This just appends a vector of ones to the dimension of your feature vector to accomodate for the bias / constant term in our hypothesis function.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `feature_transform`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use a smaller 'dummy' data to make it easier to test and debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_transformed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6b10a20c5203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdummy_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdummy_transformed_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Before feature transform:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\Learning Materilas\\Updated-Notebooks\\02 - Linear Regression\\linear_regression.py\u001b[0m in \u001b[0;36mfeature_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_transformed' is not defined"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,3)\n",
    "dummy_transformed_X = regressor.feature_transform(dummy_X)\n",
    "print(\"Before feature transform:\")\n",
    "print(dummy_X)\n",
    "print(\"Shape:\",dummy_X.shape)\n",
    "print()\n",
    "print(\"After feature transform:\")\n",
    "print(dummy_transformed_X)\n",
    "print(\"Shape:\",dummy_transformed_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output of the feature transform function.\n",
    "\n",
    "Before feature transform:\n",
    "```\n",
    "[[ 1.62434536 -0.61175641 -0.52817175]\n",
    " [-1.07296862  0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069   0.3190391 ]\n",
    " [-0.24937038  1.46210794 -2.06014071]\n",
    " [-0.3224172  -0.38405435  1.13376944]]\n",
    "Shape: (5, 3)\n",
    "```\n",
    "After feature transform:\n",
    "\n",
    "```\n",
    "[[ 1.62434536 -0.61175641 -0.52817175  1.        ]\n",
    " [-1.07296862  0.86540763 -2.3015387   1.        ]\n",
    " [ 1.74481176 -0.7612069   0.3190391   1.        ]\n",
    " [-0.24937038  1.46210794 -2.06014071  1.        ]\n",
    " [-0.3224172  -0.38405435  1.13376944  1.        ]]\n",
    "Shape: (5, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train! Compute for the weights via closed form/analytical solution\n",
    "**In `linear_regression.py`, fill in the code for the function `train_analytic`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,4)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.train_analytic(dummy_X,dummy_y)\n",
    "\n",
    "print(\"Weight vector:\")\n",
    "print(regressor.params['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected weights $W$.\n",
    "\n",
    "Weight vector:\n",
    "```\n",
    "[[ 3.58229626]\n",
    " [ 4.10375196]\n",
    " [ 4.66577618]\n",
    " [ 4.14835304]\n",
    " [ 2.50637491]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test! Predict test data with the weights computed\n",
    "**In `linear_regression.py`, fill in the code for the function `predict`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(5,4)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.train_analytic(dummy_X,dummy_y)\n",
    "predictions = regressor.predict(dummy_X)\n",
    "\n",
    "print(\"Input\",'\\t\\t\\t\\t\\t\\t\\t','Predictions')\n",
    "for i in range(dummy_X.shape[0]):\n",
    "    print(dummy_X[i],'\\t',predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "Input \t\t\t\t\t\t\t                     Predictions\n",
    "[ 1.62434536 -0.61175641 -0.52817175 -1.07296862] \t [-1.10061918]\n",
    "[ 0.86540763 -2.3015387   1.74481176 -0.7612069 ] \t [ 1.14472371]\n",
    "[ 0.3190391  -0.24937038  1.46210794 -2.06014071] \t [ 0.90159072]\n",
    "[-0.3224172  -0.38405435  1.13376944 -1.09989127] \t [ 0.50249434]\n",
    "[-0.17242821 -0.87785842  0.04221375  0.58281521] \t [ 0.90085595]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have now successfully implemented the analytical solution for Linear Regression. Let's try it out on actual (non-dummy) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.train_analytic(X_train,y_train)\n",
    "\n",
    "print(\"Weights W:\")\n",
    "print(regressor.params[\"W\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize. It helps to check how our hypothesis \"line\" looks like plotted with our data. Let's visualize using our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "X_range = np.expand_dims(np.arange(0,5000,1),1)\n",
    "y_range = regressor.predict(X_range)\n",
    "\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.plot(X_range,y_range,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a line cutting right across the middle of the green data dots, that's a great sign that our model was able to train properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test it with our own input\n",
    "\n",
    "The following input will allow us to estimate a price given a living area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Type in Q to quit.\\n\")\n",
    "while(True):\n",
    "    print(\"Living area:\")\n",
    "    a = input()\n",
    "\n",
    "    if(a == \"Q\" or a == 'q'):\n",
    "        break;\n",
    "    elif(a.isdigit()):\n",
    "        X_in = np.expand_dims([int(a)],1)\n",
    "        y_in = regressor.predict(X_in)\n",
    "        print(\"For a living area of\", a , \"sqm, the estimated price is $%0.2f \\n\" % np.squeeze(y_in))\n",
    "    else:\n",
    "        print(\"Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this out.** Try putting in a small number for our square footage. Did you see a negative price? \n",
    "\n",
    "Right now, the model predicts negative prices for small inputs. This is because our model does not know that negative prices do not make sense, it will just fit a linear function that extends to the whole range of real numbers including negative numbers. In practice you might want to perform some post processing or some other work arounds to make the predictions more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution method in a nutshell\n",
    "The analytical solution / normal equation directly solves for the optimal parameters. This will work well for some problems, but it requires taking the inverse of the matrix $X^TX$. The time complexity of an inverse operation is approximately $O(n^3)$. We also need to store all of our data $X$ in memory to be able to compute for $X^TX$. This makes it impractical to use for very large / high-dimensional datasets. \n",
    "\n",
    "Another way to solve for the parameters is using stochastic gradient descent which we will implement next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 : Iterative solution using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells show the step-by-step process of implementing the stochastic gradient descent solution of linear regression:\n",
    "\n",
    "1. Appending $w$ in the weight vector $W$ and $x$ in $X$ to account for the bias term.\n",
    "2. Initialize the parameters/weight vector $W$\n",
    "3. Calculate the cost/loss/objective function for the current parameters/weights\n",
    "4. Compute for the gradients of the loss function with respect to the parameters/weights\n",
    "5. Update the paramters/weights based on the computed gradients.\n",
    "\n",
    "Like before, we will first test and debug our code using a smaller dummy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: (Done) Pre-processing! Add in a vector of one to X to account for the bias\n",
    "We have already implemented this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Weights! Initialize the weights to a random value.\n",
    "\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `initialize_weights`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.initialize_weights(5)\n",
    "print(\"Weights vector:\")\n",
    "print(regressor.params[\"W\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Weights vector:\n",
    "[[ 0.01624345]\n",
    " [-0.00611756]\n",
    " [-0.00528172]\n",
    " [-0.01072969]\n",
    " [ 0.00865408]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 and 4: Compute for the loss and its gradients with respect to the current weights.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `loss`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "dummy_X = np.random.randn(5,5)\n",
    "dummy_y = np.random.randn(5,1)\n",
    "regressor.initialize_weights(5)\n",
    "loss, grads = regressor.loss(dummy_X, dummy_y)\n",
    "print(\"Loss :\", loss)\n",
    "print(\"grads['W'] :\",grads['W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Loss : 0.1724358913\n",
    "grads['W'] : [[ 0.48985334]\n",
    " [-0.55388664]\n",
    " [-0.29967106]\n",
    " [-0.26149345]\n",
    " [ 0.25690887]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Iteratively update the weights based on the gradient.\n",
    "\n",
    "**Open `linear_regression.py`, and fill in the code for the function `train`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "np.random.seed(1)\n",
    "dummy_X = np.random.randn(1000,5)\n",
    "dummy_y = np.sum(dummy_X*3,axis=1, keepdims=True) + np.random.randn(1000,1)*0.5\n",
    "stats = regressor.train(dummy_X,dummy_y,learning_rate=0.01,num_iters=500,batch_size=50,verbose=True)\n",
    "plt.plot(stats['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**: \n",
    "\n",
    "The loss plot should be exponentially decreasing and the loss values should be similar to the one below.\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Epoch 0 \t training RMSE: 6.6090\n",
    "Epoch 1 \t training RMSE: 5.3775\n",
    "Epoch 2 \t training RMSE: 5.0075\n",
    "Epoch 3 \t training RMSE: 3.7524\n",
    "Epoch 4 \t training RMSE: 3.6866\n",
    "iteration 100 / 500: loss 3.766868\n",
    "Epoch 5 \t training RMSE: 2.2935\n",
    "Epoch 6 \t training RMSE: 2.2515\n",
    "Epoch 7 \t training RMSE: 1.7601\n",
    "Epoch 8 \t training RMSE: 1.7724\n",
    "Epoch 9 \t training RMSE: 1.9090\n",
    "iteration 200 / 500: loss 1.001868\n",
    "Epoch 10 \t training RMSE: 1.6331\n",
    "Epoch 11 \t training RMSE: 1.1228\n",
    "Epoch 12 \t training RMSE: 1.3707\n",
    "Epoch 13 \t training RMSE: 1.0007\n",
    "Epoch 14 \t training RMSE: 1.4122\n",
    "iteration 300 / 500: loss 0.486848\n",
    "Epoch 15 \t training RMSE: 1.0572\n",
    "Epoch 16 \t training RMSE: 0.8985\n",
    "Epoch 17 \t training RMSE: 0.7991\n",
    "Epoch 18 \t training RMSE: 0.7868\n",
    "Epoch 19 \t training RMSE: 0.7851\n",
    "iteration 400 / 500: loss 0.236503\n",
    "Epoch 20 \t training RMSE: 0.7128\n",
    "Epoch 21 \t training RMSE: 0.7702\n",
    "Epoch 22 \t training RMSE: 0.7366\n",
    "Epoch 23 \t training RMSE: 0.7527\n",
    "Epoch 24 \t training RMSE: 0.5996\n",
    "iteration 500 / 500: loss 0.154280\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have now successfully implemented Linear Regression using Stochastic Gradient Descent. Let's try it out on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "regressor = LinearRegression()\n",
    "stats = regressor.train(X_train,y_train,learning_rate=1e-4,num_iters=500,batch_size=512,verbose=True)\n",
    "plt.plot(stats['loss_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things not going as expected?** If the magnitude of the output and feature values are quite big, we can easily encounter numerical overflow (or underflow if the magnitude is too small). To address this issue, it is often a good idea to normalize or standardize the data.\n",
    "\n",
    "### (Pre-processing) Standardize the features by subtracting it with its mean and then dividing by its standard deviation. \n",
    "In practice, we standardize our data to bring it to a managable magnitude. You will be encountering different standardization techniques in the future. For this notebook, we will be using the standard score. Given training data $X \\in \\mathbb{R}^{N,D}$, we compute for mean vector $\\mu_x \\in \\mathbb{R}^{1,D}$ and standard deviation vector $\\sigma_x \\in \\mathbb{R}^{1,D}$. We use $\\mu_x$ and $\\sigma_x$ to standardize both training data and validation data.  \n",
    "<br>\n",
    "Simmilarly, given training data target $Y \\in \\mathbb{R}^{N}$, we compute for mean $\\mu_y \\in \\mathbb{R}^{1}$ and standard deviation $\\sigma_y \\in \\mathbb{R}^{1}$. We use $\\mu_y$ and  $\\sigma_y$ to standardize both training target and validation target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize the training data and the validation data.\n",
    "# \n",
    "################################################################################\n",
    "\n",
    "X_train_norm = None\n",
    "y_train_norm = None\n",
    "\n",
    "X_val_norm = None\n",
    "y_val_norm = None\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "regressor = LinearRegression()\n",
    "stats = regressor.train(X_train_norm,y_train_norm,X_val_norm, y_val_norm,\n",
    "                            learning_rate=1e-2,learning_rate_decay=0.9,num_iters=1000,batch_size=512,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**\n",
    "```\n",
    "Epoch 0 \t training RMSE: 0.9630\n",
    "\t\t validation RMSE: 0.9605\n",
    "Epoch 1 \t training RMSE: 0.8200\n",
    "\t\t validation RMSE: 0.8651\n",
    "Epoch 2 \t training RMSE: 0.8288\n",
    "\t\t validation RMSE: 0.8060\n",
    "Epoch 3 \t training RMSE: 1.0526\n",
    "\t\t validation RMSE: 0.7706\n",
    "iteration 100 / 1000: loss 0.319256\n",
    "Epoch 4 \t training RMSE: 0.7171\n",
    "\t\t validation RMSE: 0.7453\n",
    "Epoch 5 \t training RMSE: 0.8004\n",
    "\t\t validation RMSE: 0.7308\n",
    "Epoch 6 \t training RMSE: 0.7045\n",
    "\t\t validation RMSE: 0.7204\n",
    "iteration 200 / 1000: loss 0.354085\n",
    "Epoch 7 \t training RMSE: 0.7163\n",
    "\t\t validation RMSE: 0.7136\n",
    "Epoch 8 \t training RMSE: 0.8864\n",
    "\t\t validation RMSE: 0.7091\n",
    "Epoch 9 \t training RMSE: 0.7350\n",
    "\t\t validation RMSE: 0.7050\n",
    "Epoch 10 \t training RMSE: 0.7186\n",
    "\t\t validation RMSE: 0.7037\n",
    "iteration 300 / 1000: loss 0.195399\n",
    "Epoch 11 \t training RMSE: 0.6950\n",
    "\t\t validation RMSE: 0.7017\n",
    "Epoch 12 \t training RMSE: 0.7377\n",
    "\t\t validation RMSE: 0.7007\n",
    "Epoch 13 \t training RMSE: 0.7426\n",
    "\t\t validation RMSE: 0.6996\n",
    "iteration 400 / 1000: loss 0.275122\n",
    "Epoch 14 \t training RMSE: 0.8269\n",
    "\t\t validation RMSE: 0.6986\n",
    "Epoch 15 \t training RMSE: 0.6815\n",
    "\t\t validation RMSE: 0.6979\n",
    "Epoch 16 \t training RMSE: 0.6626\n",
    "\t\t validation RMSE: 0.6973\n",
    "Epoch 17 \t training RMSE: 0.8149\n",
    "\t\t validation RMSE: 0.6969\n",
    "iteration 500 / 1000: loss 0.284460\n",
    "Epoch 18 \t training RMSE: 0.7000\n",
    "\t\t validation RMSE: 0.6966\n",
    "Epoch 19 \t training RMSE: 0.8664\n",
    "\t\t validation RMSE: 0.6963\n",
    "Epoch 20 \t training RMSE: 0.6585\n",
    "\t\t validation RMSE: 0.6961\n",
    "iteration 600 / 1000: loss 0.219908\n",
    "Epoch 21 \t training RMSE: 0.6556\n",
    "\t\t validation RMSE: 0.6960\n",
    "Epoch 22 \t training RMSE: 0.6476\n",
    "\t\t validation RMSE: 0.6958\n",
    "Epoch 23 \t training RMSE: 0.6376\n",
    "\t\t validation RMSE: 0.6957\n",
    "Epoch 24 \t training RMSE: 0.8272\n",
    "\t\t validation RMSE: 0.6956\n",
    "iteration 700 / 1000: loss 0.179505\n",
    "Epoch 25 \t training RMSE: 0.7812\n",
    "\t\t validation RMSE: 0.6955\n",
    "Epoch 26 \t training RMSE: 0.7189\n",
    "\t\t validation RMSE: 0.6954\n",
    "Epoch 27 \t training RMSE: 0.6691\n",
    "\t\t validation RMSE: 0.6953\n",
    "iteration 800 / 1000: loss 0.233584\n",
    "Epoch 28 \t training RMSE: 0.8886\n",
    "\t\t validation RMSE: 0.6952\n",
    "Epoch 29 \t training RMSE: 0.6723\n",
    "\t\t validation RMSE: 0.6952\n",
    "Epoch 30 \t training RMSE: 0.7169\n",
    "\t\t validation RMSE: 0.6951\n",
    "iteration 900 / 1000: loss 0.352268\n",
    "Epoch 31 \t training RMSE: 0.8394\n",
    "\t\t validation RMSE: 0.6951\n",
    "Epoch 32 \t training RMSE: 0.6996\n",
    "\t\t validation RMSE: 0.6951\n",
    "Epoch 33 \t training RMSE: 0.7227\n",
    "\t\t validation RMSE: 0.6950\n",
    "Epoch 34 \t training RMSE: 0.6987\n",
    "\t\t validation RMSE: 0.6950\n",
    "iteration 1000 / 1000: loss 0.371101\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your results slightly different? You might want to consider checking your code again. \n",
    "\n",
    "**Hint**: it may have to do with how you treat your data before predicting \n",
    "           or it may have to do with your sampling. Remember to set replacement to false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to keep track of the losses as well as both the training set performance and the validation set performance while training so we can get some insights as to how the learning process is going. We will be discussing more on error analysis later on in the course. Let's visualize the loss history as well as the RMSE history of the training set and validation set during training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "tr = plt.plot(stats['train_rmse'], label='train')\n",
    "vl = plt.plot(stats['val_rmse'], label='val')\n",
    "plt.title('Root Mean Squared Error history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.legend(('val','train'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them seem to be converging, which is usually a good sign.\n",
    "\n",
    "Now that we have trained our model, we need to convert the predicted values back to its original scale.\n",
    "<b>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:*** use $\\mu_x$ and $\\sigma_x$ you got from training to standardize your new data. Simmilarly, use $\\mu_y$ and $\\sigma_y$ to bring the predicted y values to its original scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "X_range = np.expand_dims(np.arange(0,5000,1),1)\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize the data.                                                        #\n",
    "################################################################################\n",
    "X_range_norm = None\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "y_range_norm = regressor.predict(X_range_norm)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Convert the predicted y values back to its original scale.                   #\n",
    "################################################################################\n",
    "y_range = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "plt.scatter(X_train,y_train,c='green')\n",
    "plt.plot(X_range,y_range,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now try the model on the separate test set in order to fairly evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot fitted regression line \n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Standardize your training and validation data using your training mean       #\n",
    "# standard deviation.                                                          #\n",
    "################################################################################\n",
    "X_test_norm = None\n",
    "y_test_norm = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "y_test_norm_pred = regressor.predict(X_test_norm)\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Convert the predicted y values back to its original scale.                   #\n",
    "################################################################################\n",
    "y_test_pred = None\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "plt.scatter(X_test,y_test,c='green')\n",
    "plt.plot(X_test,y_test_pred,c='blue')\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0,1500000])\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xlabel(\"Living area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((y_test_norm_pred - y_test_norm)**2))\n",
    "print(\"Test set RMSE:\",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the learning rate affect the training?\n",
    "In practice, we do not know the optimal learning rate in advanced. One way to search for a good learning rate is to try different values of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "alphas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,10))\n",
    "for alpha in alphas:\n",
    "    regressor = LinearRegression()\n",
    "    stats = regressor.train(X_train_norm,y_train_norm,X_val_norm, y_val_norm,\n",
    "                                learning_rate=alpha,learning_rate_decay=0.9,num_iters=1000,batch_size=512,verbose=False)\n",
    "    plt.subplot(2,3,plt_ctr)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history (alpha = '+str(alpha)+')')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt_ctr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of the loss plot the optimal learning rate would be near 0.01. Later on in the course we will be discussing how to efficiently tune hyperparameters such as the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment!\n",
    "We have only used one feature so far. Try out other feature combinations to improve the model's performance. Feel free to try out other things as well. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> fin </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
