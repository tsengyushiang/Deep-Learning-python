{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, you implemented linear models for regression and multi-class classification. In this exercise, you will combine those ideas to create neural networks with arbitrary number of layers to perform multi-class classification, also called multi-layered perceptrons.\n",
    "\n",
    "**You will learn to:**\n",
    "- Compute Numerical Gradients to be used as gradient checkers\n",
    "- Build the general architecture of a Neural Network Model consisting of fully connected layers.\n",
    "    - Initializing Parameters/Weights of each layer\n",
    "    - Implement the forward pass\n",
    "        - forward pass of fully connected layers\n",
    "        - forward pass of sigmoid activation function\n",
    "        - forward pass of tanh activation function\n",
    "        - forward pass of ReLU activation function\n",
    "    - Implement the backward pass to compute for gradients\n",
    "        - backward pass of fully connected layers\n",
    "        - backward pass of sigmoid activation function\n",
    "        - backward pass of tanh activation function\n",
    "        - backward pass of ReLU activation function\n",
    "    - Calculating the Cost/Loss/Objective Function\n",
    "    - Implement gradient descent to update the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient\n",
    "We will be stacking multiple layers on top of one another to generate more complex hypothesis functions. Solving for the analytical gradients of each layer's parameters is no longer trivial and will be prone to errors. Fortunately, there is an easy way to compute gradients numerically. It is very slow to be used to train neural networks but it is very useful in debugging our analytically derived gradients.\n",
    "\n",
    "**Open `gradient_checker.py`, and implement `compute_numerical_gradient`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_checker import compute_numerical_gradient, relative_error\n",
    "np.random.seed(1)\n",
    "\n",
    "# creates a dummy loss function\n",
    "def dummy_loss_function(X, y, W):\n",
    "    N, D = X.shape\n",
    "    loss = 0.5 * np.mean((X.dot(W) - y)**2)\n",
    "    \n",
    "    grad = {}\n",
    "    grad['W'] = np.dot(X.T, X.dot(W) - y) / N\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "X_dummy = np.random.randn(3,5)\n",
    "y_dummy = np.random.randn(3,1)\n",
    "W_dummy = np.random.randn(5,1)\n",
    "\n",
    "# solves for the analytical gradient.\n",
    "loss, grad = dummy_loss_function(X_dummy,y_dummy,W_dummy)\n",
    "\n",
    "# lambda functions are anonymous functions defined without a name. We use this to\n",
    "# pass in a function that has only W as its parameter and everything else is fixed. \n",
    "# Since the compute_numerical_gradient expects a function that outputs a scalar value,\n",
    "# we return only the first element which corresponds to the loss value\n",
    "numerical_gradients = compute_numerical_gradient(lambda W: dummy_loss_function(X_dummy, y_dummy, W)[0], W_dummy)\n",
    "\n",
    "print(\"Analytical Gradients\")\n",
    "print(grad['W'])\n",
    "\n",
    "print(\"Numerical Gradients\")\n",
    "print(numerical_gradients)\n",
    "\n",
    "print(\"Relative Error\")\n",
    "print(relative_error(grad['W'], numerical_gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "Analytical Gradients\n",
    "[[-0.74692649]\n",
    " [ 0.54694129]\n",
    " [-0.76442224]\n",
    " [-0.2044723 ]\n",
    " [ 0.35616946]]\n",
    "Numerical Gradients\n",
    "[[-0.74692649]\n",
    " [ 0.54694129]\n",
    " [-0.76442224]\n",
    " [-0.2044723 ]\n",
    " [ 0.35616946]]\n",
    "Relative Error\n",
    "1.63980427129e-11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "We will first use a toy dataset, so we can visualize our data and model's predictions in 2D. Below are two functions that generates circular data and spiral data, both of which are not linearly separable. We will use the circle data by default but feel free to experiment with the spiral data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_circle_data(num_points):\n",
    "    r = np.random.uniform(0,2,num_points)\n",
    "    theta = np.random.uniform(0,2*np.pi,num_points)\n",
    "    inner_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "    \n",
    "    r = np.random.uniform(5,7,num_points)\n",
    "    theta = 2*np.pi*np.arange(num_points)/num_points\n",
    "    outer_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "\n",
    "    X = np.concatenate((inner_circle,outer_circle),axis=0)\n",
    "    y = np.concatenate((np.ones(num_points),np.zeros(num_points)),axis=0)\n",
    "    \n",
    "    randIdx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randIdx)\n",
    "    \n",
    "    X = X[randIdx]\n",
    "    y = y[randIdx].astype(int)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def generate_dummy_spiral_data(num_points, num_spiral):\n",
    "    r = np.random.uniform(-0.1, 0.1,num_points) + 5*np.arange(num_points)/num_points\n",
    "    theta = np.random.uniform(-0.1, 0.1,num_points) + 2*np.pi*1.25*np.arange(num_points)/num_points\n",
    "    spiral = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "    y = np.ones(num_points)\n",
    "\n",
    "    for i in range(1,num_spiral+1):\n",
    "        r = np.random.uniform(-0.1, 0.1,num_points) + 5*np.arange(num_points)/num_points\n",
    "        theta = np.random.uniform(-0.1, 0.1,num_points) + 2*np.pi*1.25*np.arange(num_points)/num_points + 2*i*np.pi/num_spiral\n",
    "        tmp_spiral = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "\n",
    "        spiral = np.concatenate((spiral,tmp_spiral),axis=0)\n",
    "        if i % 2 == 1:\n",
    "            y = np.concatenate((y,np.zeros(num_points)),axis=0)\n",
    "        else:\n",
    "            y = np.concatenate((y,np.ones(num_points)),axis=0)\n",
    "\n",
    "    randIdx = np.arange(spiral.shape[0])\n",
    "    np.random.shuffle(randIdx)\n",
    "\n",
    "    X = spiral[randIdx]\n",
    "    y = y[randIdx].astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_circle,y_circle = generate_dummy_circle_data(100)\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_circle[:,0],X_circle[:,1],c=y_circle)\n",
    "\n",
    "X_spiral,y_spiral = generate_dummy_spiral_data(100,2)\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_spiral[:,0],X_spiral[:,1],c=y_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in \\mathbb{R}^{N,D}$ - like the multinomial logistic regression, our data is also represented as a matrix with $N$ rows and $D$ columns, where each row is a $D$-dimensional feature vector representing an instance / example in our dataset $(x_i \\in \\mathbb{R}^D)$. In this particular example, $D=2$.\n",
    "\n",
    "$y \\in \\{0,..,C\\}^N$ - Given $C$ distinct classes, the prediction target is represented as a vector of length $N$ and each example $y_i$ is a scalar that can take on a value from 0 to $C$.\n",
    "\n",
    "**Note that the math expresses our target variable $y_i$ as a one-hot encoding vector, where it has a value of 1 corresponding to the correct class and 0 everywhere else. In practice, we represent $y_i$ as a scalar value denoting the index of the correct class instead. This is because it is not computationally and memory efficient to treat each $y_i$ as a vector, specially for large number of classes, when almost all of its values are 0.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of X:\", X_circle.shape)\n",
    "print(\"The shape of y:\", y_circle.shape)\n",
    "print(\"\\nFirst 5 examples:\")\n",
    "for i in range(5):\n",
    "    print(\"X[{}] = {}\\t y[{}] = {}\".format(i, X_circle[i], i, y_circle[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize Weights! We initialize the weights with small random values and the biases are initialized to zero.\n",
    "Open `neural_networks.py`, and fill in the code for the function `initialize_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(hidden_size=6,num_classes=3)\n",
    "net.initialize_weights(input_dim=5)\n",
    "\n",
    "for param in net.params:\n",
    "    print(\"Shape of\",param, net.params[param].shape)\n",
    "\n",
    "for param in net.params:\n",
    "    print(param, net.params[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "Shape of W1 (5, 6)\n",
    "Shape of b1 (6,)\n",
    "Shape of W2 (6, 3)\n",
    "Shape of b2 (3,)\n",
    "W1 [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408 -0.02301539]\n",
    " [ 0.01744812 -0.00761207  0.00319039 -0.0024937   0.01462108 -0.02060141]\n",
    " [-0.00322417 -0.00384054  0.01133769 -0.01099891 -0.00172428 -0.00877858]\n",
    " [ 0.00042214  0.00582815 -0.01100619  0.01144724  0.00901591  0.00502494]\n",
    " [ 0.00900856 -0.00683728 -0.0012289  -0.00935769 -0.00267888  0.00530355]]\n",
    "b1 [ 0.  0.  0.  0.  0.  0.]\n",
    "W2 [[-0.00691661 -0.00396754 -0.00687173]\n",
    " [-0.00845206 -0.00671246 -0.00012665]\n",
    " [-0.0111731   0.00234416  0.01659802]\n",
    " [ 0.00742044 -0.00191836 -0.00887629]\n",
    " [-0.00747158  0.01692455  0.00050808]\n",
    " [-0.00636996  0.00190915  0.02100255]]\n",
    "b2 [ 0.  0.  0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement forward pass of a Fully Connected Layer (Also often called affine, linear, or dense layer)\n",
    "Open `neural_networks.py`, and fill in the code for the function `fully_connected_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(num_layers=2,num_classes=2, hidden_size=3)\n",
    "net.initialize_weights(input_dim=2)\n",
    "\n",
    "out, cache = net.fully_connected_forward(X_circle[:5],net.params['W1'], net.params['b1'])\n",
    "print(\"Fully Connected Layer Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "Fully Connected Layer Output:\n",
    "[[ -5.43922962e-05  -2.95825783e-03   1.71318792e-02]\n",
    " [  7.67884387e-02  -5.66338288e-02   1.34260549e-01]\n",
    " [ -3.38834903e-02   1.55489850e-02  -5.00005074e-03]\n",
    " [  1.64916743e-02   1.06056517e-02  -1.01981795e-01]\n",
    " [ -7.82087392e-03   4.93558593e-03  -8.89102840e-03]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement backward pass of a Fully Connected Layer.\n",
    "Open `neural_networks.py`, and fill in the code for the function `fully_connected_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "W1 = np.random.randn(2,5)\n",
    "b1 = np.random.randn(5)\n",
    "dUpper = np.random.randn(5, 5)\n",
    "\n",
    "out, cache = net.fully_connected_forward(X_circle[:5],W1, b1)\n",
    "\n",
    "dX, dW, db = net.fully_connected_backward(dUpper,cache)\n",
    "\n",
    "dX_num = compute_numerical_gradient(lambda X: np.sum(dUpper*net.fully_connected_forward(X, W1, b1)[0])\n",
    "                           , X_circle[:5])\n",
    "\n",
    "dW_num = compute_numerical_gradient(lambda W: np.sum(dUpper*net.fully_connected_forward(X_circle[:5], W, b1)[0])\n",
    "                           , W1)\n",
    "\n",
    "db_num = compute_numerical_gradient(lambda b: np.sum(dUpper*net.fully_connected_forward(X_circle[:5],W1, b)[0])\n",
    "                           ,  b1)\n",
    "\n",
    "print(\"Gradient dX Relative Error\",relative_error(dX, dX_num))\n",
    "print(\"Gradient dW Relative Error\",relative_error(dW, dW_num))\n",
    "print(\"Gradient db Relative Error\",relative_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "The relative errors of the gradients should be less than $10^{-8}$. The values may vary depending on your implementation.\n",
    "\n",
    "Largest Relative Errors in our implementation:\n",
    "```\n",
    "Gradient dX Relative Error 1.63516379584e-10\n",
    "Gradient dW Relative Error 5.41617890346e-09\n",
    "Gradient db Relative Error 5.70914573891e-11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the forward pass of the Sigmoid activation function\n",
    "Open `neural_networks.py`, and fill in the code for the function `sigmoid_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "out, cache = net.sigmoid_forward(np.random.randn(5,5))\n",
    "print(\"Sigmoid Layer Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "Sigmoid Layer Output:\n",
    "[[ 0.83539354  0.35165864  0.3709434   0.25483894  0.70378922]\n",
    " [ 0.09099561  0.85129722  0.31838429  0.57909005  0.43797848]\n",
    " [ 0.81185487  0.11303172  0.42008677  0.40514941  0.75653387]\n",
    " [ 0.24976027  0.45699943  0.29362176  0.51055187  0.64171493]\n",
    " [ 0.2496239   0.75854586  0.71127629  0.62304533  0.71112537]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement the backward pass of the Sigmoid activation function\n",
    "Open `neural_networks.py`, and fill in the code for the function `sigmoid_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "dUpper = np.random.randn(5, 5)\n",
    "dummy_input = np.random.randn(5,5)\n",
    "out, cache = net.sigmoid_forward(dummy_input)\n",
    "\n",
    "dSigmoid = net.sigmoid_backward(dUpper,cache)\n",
    "\n",
    "dSigmoid_num = compute_numerical_gradient(lambda X: np.sum(dUpper*net.sigmoid_forward(X)[0])\n",
    "                           , dummy_input)\n",
    "\n",
    "print(\"Gradient dSigmoid Relative Error\",relative_error(dSigmoid, dSigmoid_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "The relative errors of the gradients should be less than $10^{-8}$. The values may vary depending on your implementation.\n",
    "\n",
    "Largest Relative Error in our implementation:\n",
    "```\n",
    "Gradient dSigmoid Relative Error 2.8230695697e-10\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement softmax cross entropy loss layer\n",
    "Lets first implement softmax function that converts raw scores to probabilities.\n",
    "\n",
    "Open `neural_networks.py`, and fill in the code for the function `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "probs = net.softmax(np.array([[1001,1002,1003,1004,1005],[3,4,5,6,7]]))\n",
    "print(\"Probabilities of belonging to each class\")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output:\n",
    "```\n",
    "Probabilities of belonging to each class\n",
    "[[ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]\n",
    " [ 0.01165623  0.03168492  0.08612854  0.23412166  0.63640865]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the softmax cross entropy loss and compute for its gradients. Since this is applied in the last layer, we do not need to worry about the gradients coming from after this layer.\n",
    "\n",
    "Open `neural_networks.py`, and fill in the code for the function `softmax_cross_entropy_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "loss, dloss = net.softmax_cross_entropy_loss(np.random.randn(4,6),np.random.randint(0,6,size=4))\n",
    "print(\"Softmax Cross-entropy Loss\")\n",
    "print(loss)\n",
    "print()\n",
    "print(\"Gradient of the loss with respect to the scores\")\n",
    "print(dloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output:\n",
    "```\n",
    "Softmax Cross-entropy Loss\n",
    "3.23430146928\n",
    "\n",
    "Gradient of the loss with respect to the scores\n",
    "[[ 0.14058054  0.01502445  0.01633424  0.0094732   0.06581467 -0.24722709]\n",
    " [ 0.11190472  0.00913058  0.02689325 -0.23476698  0.0843474   0.00249103]\n",
    " [ 0.02967359 -0.22210018  0.12728696  0.01363695  0.03447541  0.01702727]\n",
    " [ 0.02501532  0.04295228 -0.24202226  0.07533903  0.0590784   0.03963723]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a simple network!\n",
    "Now that we have implemented two different types of layers, we can now build a simple neural network consisting of fully connected layers with sigmoid activations.\n",
    "\n",
    "We will follow a simple feed forward neural network architecture as shown below:\n",
    "```\n",
    "Repeat for (Number of layers - 1) \n",
    "    [Fully Connected Layer] \n",
    "    [Activation Layer]\n",
    "\n",
    "[Fully Connected Layer] -> (Output layer)\n",
    "```\n",
    "\n",
    "**Open `neural_networks.py`, and fill in the code for the function `network_forward`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(num_layers=2, num_classes=3, hidden_size=10, hidden_activation_fn=\"sigmoid\")\n",
    "net.initialize_weights(5,std_dev=0.1)\n",
    "\n",
    "scores, cache_list  = net.network_forward(np.random.randn(5,5)*10)\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output:\n",
    "```\n",
    "Forward Pass:\n",
    "[[ 0.19910154 -0.11077738  0.24974865]\n",
    " [ 0.26508479 -0.18731063  0.24634693]\n",
    " [ 0.3482853  -0.25223747 -0.06853373]\n",
    " [ 0.30067375 -0.14904403  0.18594437]\n",
    " [ 0.20967705 -0.07065423  0.10956096]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute for the losses corresponding to the current parameters.\n",
    "Implement `loss` function which should output the losses as well as its the gradients. Note that the gradient computation of the layers is implemented in a separate function `network_backward` which you should also implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(num_layers=2,num_classes=2, hidden_size=10, hidden_activation_fn=\"sigmoid\")\n",
    "net.initialize_weights(input_dim=2, std_dev=0.5)\n",
    "loss, grads = net.loss(X_circle[:5]*10, y_circle[:5], lambda_reg=0.0)\n",
    "print(\"Loss\", loss )\n",
    "for param in net.params:\n",
    "    f = lambda W: net.loss(X_circle[:5]*10, y_circle[:5], lambda_reg=0.0)[0]\n",
    "    param_grad_num = compute_numerical_gradient(f, net.params[param])\n",
    "    # Uncomment this if you want to print out the actual values for debugging.\n",
    "    # print(param + \"_numerical\", param_grad_num)\n",
    "    #print(param + \"_analytical\",grads[param])\n",
    "    print('{} Relative Error: {}'.format(param, relative_error(param_grad_num, grads[param])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative errors of the gradients should be less than $10^{-8}$. The values may vary depending on your implementation.\n",
    "\n",
    "Largest Relative Errors in our implementation:\n",
    "```\n",
    "Loss 1.036316724158344\n",
    "W1 Relative Error: 1.565689660633177e-08\n",
    "b1 Relative Error: 8.285555392599544e-09\n",
    "W2 Relative Error: 8.125385495575136e-11\n",
    "b2 Relative Error: 2.0095447739463423e-11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure l2 reguralization is working\n",
    "Add l2 reguralization in your gradient and loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(num_layers=2,num_classes=2, hidden_size=10, hidden_activation_fn=\"sigmoid\")\n",
    "net.initialize_weights(input_dim=2, std_dev=0.5)\n",
    "loss, grads = net.loss(X_circle[:5]*10, y_circle[:5], lambda_reg=0.5)\n",
    "print(\"Loss Value:\",loss)\n",
    "for param in net.params:\n",
    "    f = lambda W: net.loss(X_circle[:5]*10, y_circle[:5], lambda_reg=0.5)[0]\n",
    "    param_grad_num = compute_numerical_gradient(f, net.params[param])\n",
    "    # Uncomment this if you want to print out the actual values for debugging.\n",
    "    #print(param + \"_numerical\", param_grad_num)\n",
    "    #print(param + \"_analytical\",grads[param])\n",
    "    print('{} Relative Error: {}'.format(param, relative_error(param_grad_num, grads[param])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "```\n",
    "Loss Value: 3.3825130998167294\n",
    "W1 Relative Error: 1.59111176317992e-08\n",
    "b1 Relative Error: 8.285555392599544e-09\n",
    "W2 Relative Error: 6.406107040317305e-10\n",
    "b2 Relative Error: 6.804149678991242e-11\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = X_circle\n",
    "y = y_circle\n",
    "\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"sigmoid\")\n",
    "loss_history = net.train(X, y, learning_rate=0.9, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_train_pred = net.predict(X)\n",
    "print(\"Train accuracy: {} %\".format(np.mean(Y_train_pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize the training process!\n",
    "The code below will visualize the decision boundary (left) and the transformations (right) that the network learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"sigmoid\")\n",
    "net.initialize_weights(X.shape[1],1)\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                     np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "\n",
    "x1, y1 = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                     np.arange(y_min, y_max, 1))\n",
    "\n",
    "grid_x = np.squeeze(np.stack((x1.ravel(),y1.ravel()))).T\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i in range(300):\n",
    "    \n",
    "    net.train_step(X, y, learning_rate=1, lambda_reg=0.0, batch_size=10)\n",
    "    if i % 10 == 0:\n",
    "\n",
    "        Z = net.predict(x_test)\n",
    "\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax1.clear()\n",
    "        ax1.contourf(xx, yy, Z)\n",
    "        ax1.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "        \n",
    "        Z, sc= net.predict(grid_x, return_scores=True)\n",
    "\n",
    "        ax2.clear()\n",
    "        for i in range(12):\n",
    "            ax2.plot(sc[i*x1.shape[0]:(i+1)*x1.shape[0],0],sc[i*x1.shape[0]: (i+1)*x1.shape[0],1], \"gray\", alpha=0.5)\n",
    "            ax2.plot(sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),0],sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),1], \"gray\", alpha=0.5)\n",
    "        ax2.scatter(sc[:,0],sc[:,1], c = Z, cmap=\"jet\")\n",
    "        \n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's try other activation functions and see how it affects the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the forward pass of the Tanh activation function\n",
    "Open `neural_networks.py`, and fill in the code for the function `tanh_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "out, cache = net.tanh_forward(np.random.randn(5,5))\n",
    "print(\"Tanh Layer Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "Tanh Layer Output:\n",
    "[[ 0.92525207 -0.5453623  -0.48398233 -0.79057703  0.69903334]\n",
    " [-0.98015695  0.94078216 -0.6417873   0.30863781 -0.24432671]\n",
    " [ 0.89806123 -0.96803916 -0.31169093 -0.36622326  0.81230541]\n",
    " [-0.80045996 -0.17073944 -0.70534482  0.04218869  0.52470857]\n",
    " [-0.80072132  0.8159986   0.71707154  0.46407656  0.71671439]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement the backward pass of the Tanh activation function\n",
    "Open `neural_networks.py`, and fill in the code for the function `tanh_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "dUpper = np.random.randn(5, 5)\n",
    "dummy_input = np.random.randn(5,5)\n",
    "out, cache = net.tanh_forward(dummy_input)\n",
    "\n",
    "dTanh = net.tanh_backward(dUpper,cache)\n",
    "\n",
    "dTanh_num = compute_numerical_gradient(lambda X: np.sum(dUpper*net.tanh_forward(X)[0])\n",
    "                           , dummy_input)\n",
    "\n",
    "print(\"Gradient dTanh Relative Error\",relative_error(dTanh, dTanh_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "The relative errors of the gradients should be less than $10^{-8}$. The values may vary depending on your implementation.\n",
    "\n",
    "Largest Relative Error in our implementation:\n",
    "```\n",
    "Gradient dTanh Relative Error 1.48817404755e-09\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the decision boundaries and transformations under Tanh Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"tanh\")\n",
    "net.initialize_weights(X.shape[1],1)\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                     np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "\n",
    "x1, y1 = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                     np.arange(y_min, y_max, 1))\n",
    "\n",
    "grid_x = np.squeeze(np.stack((x1.ravel(),y1.ravel()))).T\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i in range(300):\n",
    "    \n",
    "    net.train_step(X, y, learning_rate=1, lambda_reg=0.0, batch_size=10)\n",
    "    if i % 10 == 0:\n",
    "\n",
    "        Z = net.predict(x_test)\n",
    "\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax1.clear()\n",
    "        ax1.contourf(xx, yy, Z)\n",
    "        ax1.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "        \n",
    "        Z, sc= net.predict(grid_x, return_scores=True)\n",
    "\n",
    "        ax2.clear()\n",
    "        for i in range(12):\n",
    "            ax2.plot(sc[i*x1.shape[0]:(i+1)*x1.shape[0],0],sc[i*x1.shape[0]: (i+1)*x1.shape[0],1], \"gray\", alpha=0.5)\n",
    "            ax2.plot(sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),0],sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),1], \"gray\", alpha=0.5)\n",
    "        ax2.scatter(sc[:,0],sc[:,1], c = Z, cmap=\"jet\")\n",
    "        \n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement the forward pass of ReLU\n",
    "Open `neural_networks.py`, and fill in the code for the function `relu_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "out, cache = net.relu_forward(np.random.randn(5,5))\n",
    "print(\"ReLU Layer Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "    \n",
    "Expected Output\n",
    "```\n",
    "ReLU Layer Output:\n",
    "[[ 1.62434536  0.          0.          0.          0.86540763]\n",
    " [ 0.          1.74481176  0.          0.3190391   0.        ]\n",
    " [ 1.46210794  0.          0.          0.          1.13376944]\n",
    " [ 0.          0.          0.          0.04221375  0.58281521]\n",
    " [ 0.          1.14472371  0.90159072  0.50249434  0.90085595]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement the backward pass of ReLU\n",
    "Open `neural_networks.py`, and fill in the code for the function `relu_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork()\n",
    "dUpper = np.random.randn(5, 5)\n",
    "dummy_input = np.random.randn(5,5)\n",
    "out, cache = net.relu_forward(dummy_input)\n",
    "\n",
    "dRelu = net.relu_backward(dUpper,cache)\n",
    "\n",
    "dRelu_num = compute_numerical_gradient(lambda X: np.sum(dUpper*net.relu_forward(X)[0])\n",
    "                           , dummy_input)\n",
    "\n",
    "print(\"Gradient dRelu Relative Error\",relative_error(dRelu, dRelu_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "The relative errors of the gradients should be less than $10^{-8}$. The values may vary depending on your implementation.\n",
    "\n",
    "Largest Relative Error in our implementation:\n",
    "```\n",
    "Gradient dRelu Relative Error 5.71858102885e-11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the decision boundaries and transformations under ReLU Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"relu\")\n",
    "net.initialize_weights(X.shape[1],1)\n",
    "fig = plt.figure(figsize=(13,8))\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                     np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "\n",
    "x1, y1 = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                     np.arange(y_min, y_max, 1))\n",
    "\n",
    "grid_x = np.squeeze(np.stack((x1.ravel(),y1.ravel()))).T\n",
    "x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i in range(300):\n",
    "    \n",
    "    net.train_step(X, y, learning_rate=1, lambda_reg=0.0, batch_size=10)\n",
    "    if i % 10 == 0:\n",
    "\n",
    "        Z = net.predict(x_test)\n",
    "\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax1.clear()\n",
    "        ax1.contourf(xx, yy, Z)\n",
    "        ax1.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "        \n",
    "        Z, sc= net.predict(grid_x, return_scores=True)\n",
    "\n",
    "        ax2.clear()\n",
    "        for i in range(12):\n",
    "            ax2.plot(sc[i*x1.shape[0]:(i+1)*x1.shape[0],0],sc[i*x1.shape[0]: (i+1)*x1.shape[0],1], \"gray\", alpha=0.5)\n",
    "            ax2.plot(sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),0],sc[np.arange(i,x1.shape[1]**2,x1.shape[1]),1], \"gray\", alpha=0.5)\n",
    "        ax2.scatter(sc[:,0],sc[:,1], c = Z, cmap=\"jet\")\n",
    "        \n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can compare the loss curves across the different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"sigmoid\")\n",
    "loss_history_sigmoid = net.train(X, y, learning_rate=0.8, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"tanh\")\n",
    "loss_history_tanh = net.train(X, y, learning_rate=0.8, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "\n",
    "net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=6, hidden_activation_fn=\"relu\")\n",
    "loss_history_relu = net.train(X, y, learning_rate=0.8, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sig = plt.plot(loss_history_sigmoid, label='sigmoid')\n",
    "vl = plt.plot(loss_history_tanh, label='tanh')\n",
    "relu = plt.plot(loss_history_relu, label='ReLU')\n",
    "\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also see how the size of the hidden layer affects the decision boundary. Let's start with sigmoid activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,10))\n",
    "for h_size in range(1,13, 2):\n",
    "    net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=h_size, hidden_activation_fn=\"sigmoid\")\n",
    "    loss_history_sigmoid = net.train(X, y, learning_rate=1, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "    x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "    \n",
    "    Z = net.predict(x_test).reshape(xx.shape)\n",
    "    plt.subplot(2,3,plt_ctr)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "    plt.title(\"hidden size = \"+str(h_size))\n",
    "    plt_ctr += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,10))\n",
    "for h_size in range(1,13, 2):\n",
    "    net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=h_size, hidden_activation_fn=\"tanh\")\n",
    "    loss_history_sigmoid = net.train(X, y, learning_rate=1, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "    x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "    \n",
    "    Z = net.predict(x_test).reshape(xx.shape)\n",
    "    plt.subplot(2,3,plt_ctr)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "    plt.title(\"hidden size = \"+str(h_size))\n",
    "    plt_ctr += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,10))\n",
    "for h_size in range(1,13, 2):\n",
    "    net = NeuralNetwork(num_layers=2, num_classes=2, hidden_size=h_size, hidden_activation_fn=\"relu\")\n",
    "    loss_history_sigmoid = net.train(X, y, learning_rate=1, lambda_reg=0.0, num_iters=1000, batch_size=10, verbose=False)\n",
    "    \n",
    "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "\n",
    "    x_test = np.squeeze(np.stack((xx.ravel(),yy.ravel()))).T\n",
    "    \n",
    "    Z = net.predict(x_test).reshape(xx.shape)\n",
    "    plt.subplot(2,3,plt_ctr)\n",
    "    plt.contourf(xx, yy, Z)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y,cmap=\"jet\", edgecolors='black')\n",
    "    plt.title(\"hidden size = \"+str(h_size))\n",
    "    plt_ctr += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
